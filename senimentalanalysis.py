# -*- coding: utf-8 -*-
"""senimentalAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19GHVX-Pj-UF2_fHmoKT_2o-ii71e8KCk
"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from nltk.stem import PorterStemmer
import re
import pickle
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier



#load data
data = pd.read_csv("amazon_alexa.tsv",delimiter='\t',quoting =3)
print(f"Datset shape : {data.shape}")

data.head()

#null check
data.isnull().sum()

data[data['verified_reviews'].isna()==True]

data.dropna(inplace=True)

print(f"after dropping NAN values:{data.shape}")

data['length'] = data['verified_reviews'].apply(len)

data.head()

data.dtypes

print(f"Rating vlaue count: \n{data['rating'].value_counts()}")

data['rating'].value_counts().plot.bar(color ='blue')
plt.title('Rating distribution')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

cv = CountVectorizer(stop_words="english")
words = cv.fit_transform(data.verified_reviews)

reviews = " ".join([review for review in data['verified_reviews']])

wc = WordCloud(background_color ='white',max_words = 50)

plt.figure(figsize =(10,10))
plt.imshow(wc.generate(reviews))
plt.title('Wordcloud for all review',fontsize =10)
plt.axis('off')
plt.show()

#ToDO
#Replace any non alphabet character with a space
#convert lower case and split the word
#Iterate over the individual words and if it is not a stopword then add the stemmed form of the word to the corpus

corpus = []
stemmer = PorterStemmer()
for i in range(0,data.shape[0]):
  review = re.sub('[^a-zA-Z]', ' ', data.iloc[i]['verified_reviews'])
  review =review.lower().split()
  review = [stemmer.stem(word) for word in review if not word in STOPWORDS]
  review = " ".join(review)
  corpus.append(review)

cv = CountVectorizer(max_features= 2500)
X = cv.fit_transform(corpus).toarray()
Y = data['feedback'].values

#splitting 30% testing data
X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size =0.3,random_state =15)
print(f"X train :{X_train.shape}")
print(f"Y train :{Y_train.shape}")
print(f"X test :{X_test.shape}")
print(f"X test :{X_test.shape}")

"""print(f"X shape:{X.shape}")
print(f"y shape: {Y.shape}")
"""

print(f"X train max value: {X_train.max()}")
print(f"X test max value: {X_test.max()}")

scaler = MinMaxScaler()

X_train_scl = scaler.fit_transform(X_train)
X_test_scl = scaler.transform(X_test)

model_rf = RandomForestClassifier()
model_rf.fit(X_train_scl, Y_train)

print("Training Accuracy :", model_rf.score(X_train_scl, Y_train))
print("Testing Accuracy :", model_rf.score(X_test_scl, Y_test))

